{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "'''\n",
    "get kaggle dataset\n",
    "'''\n",
    "!pip install -q kaggle\n",
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle datasets list\n",
    "!kaggle competitions download -c word2vec-nlp-tutorial\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o ./word2vec-nlp-tutorial.zip\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o ./labeledTrainData.tsv.zip\n",
    "!unzip -o ./testData.tsv.zip\n",
    "!unzip -o ./unlabeledTrainData.tsv.zip\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import csv\n",
    "\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "word_net_lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "clear_output()\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go', 'to', 'school', 'right', 'now']\n",
      "[('Go', 'VB'), ('to', 'TO'), ('school', 'NN'), ('right', 'RB'), ('now', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Go to school right now'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False, lemmalization=False):\n",
    "    \n",
    "    # remove HTML tag\n",
    "    review_text = bs.BeautifulSoup(review).get_text()\n",
    "\n",
    "    # make non-English become space\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "    \"\"\"\n",
    "    lemmalization\n",
    "    \"\"\"\n",
    "    if lemmalization:\n",
    "        tagged_sentences = pos_tag(words)\n",
    "        lemma_senetence = []\n",
    "        for tag in tagged_sentences:\n",
    "            pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "            lemma_senetence.append(word_net_lemmatizer.lemmatize(tag[0], pos=pos))\n",
    "        \n",
    "        words = lemma_senetence\n",
    "    # for word in meaningful_words:\n",
    "    #     word = word_net_lemmatizer.lemmatize(word, 'v')\n",
    "\n",
    "    # remove stopword\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_sentences(review, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    for raw_sentence in raw_sentences:\n",
    "\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords, lemmalization=True))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  sentiment                                             review\n",
      "0  5814_8          1  With all this stuff going down at the moment w...\n",
      "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "        id                                             review\n",
      "0   9999_0  Watching Time Chasers, it obvious that it was ...\n",
      "1  45057_0  I saw this film about 20 years ago and remembe...\n",
      "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...\n",
      "['the', 'classic', 'war', 'of', 'the', 'worlds', 'by', 'timothy', 'hines', 'is', 'a', 'very', 'entertaining', 'film', 'that', 'obviously', 'goes', 'to', 'great', 'effort', 'and', 'lengths', 'to', 'faithfully', 'recreate', 'h', 'g', 'wells', 'classic', 'book', 'mr', 'hines', 'succeeds', 'in', 'doing', 'so', 'i', 'and', 'those', 'who', 'watched', 'his', 'film', 'with', 'me', 'appreciated', 'the', 'fact', 'that', 'it', 'was', 'not', 'the', 'standard', 'predictable', 'hollywood', 'fare', 'that', 'comes', 'out', 'every', 'year', 'e', 'g', 'the', 'spielberg', 'version', 'with', 'tom', 'cruise', 'that', 'had', 'only', 'the', 'slightest', 'resemblance', 'to', 'the', 'book', 'obviously', 'everyone', 'looks', 'for', 'different', 'things', 'in', 'a', 'movie', 'those', 'who', 'envision', 'themselves', 'as', 'amateur', 'critics', 'look', 'only', 'to', 'criticize', 'everything', 'they', 'can', 'others', 'rate', 'a', 'movie', 'on', 'more', 'important', 'bases', 'like', 'being', 'entertained', 'which', 'is', 'why', 'most', 'people', 'never', 'agree', 'with', 'the', 'critics', 'we', 'enjoyed', 'the', 'effort', 'mr', 'hines', 'put', 'into', 'being', 'faithful', 'to', 'h', 'g', 'wells', 'classic', 'novel', 'and', 'we', 'found', 'it', 'to', 'be', 'very', 'entertaining', 'this', 'made', 'it', 'easy', 'to', 'overlook', 'what', 'the', 'critics', 'perceive', 'to', 'be', 'its', 'shortcomings']\n"
     ]
    }
   ],
   "source": [
    "labeled_df = pd.read_csv('./labeledTrainData.tsv', sep='\\t', header=0)\n",
    "unlabeled_df = pd.read_csv('./unlabeledTrainData.tsv', sep='\\t', header=0, on_bad_lines='skip')\n",
    "test_df = pd.read_csv('./testData.tsv', sep='\\t', header=0)\n",
    "# print(labeled_df.iloc[1][2])\n",
    "words = review_to_wordlist(labeled_df.iloc[1][2])\n",
    "print(labeled_df.head(3))\n",
    "print(unlabeled_df.head(3))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['with', 'all', 'this', 'stuff', 'go', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'start', 'listen', 'to', 'his', 'music', 'watch', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watch', 'the', 'wiz', 'and', 'watch', 'moonwalker', 'again'], ['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'think', 'be', 'really', 'cool', 'in', 'the', 'eighty', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'be', 'guilty', 'or', 'innocent'], ['moonwalker', 'be', 'part', 'biography', 'part', 'feature', 'film', 'which', 'i', 'remember', 'go', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'be', 'originally', 'release']]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "for review in labeled_df['review']:\n",
    "    sentences += review_to_sentences(review)\n",
    "\n",
    "for review in unlabeled_df['review']:\n",
    "    sentences += review_to_sentences(review)\n",
    "\n",
    "for review in test_df['review']:\n",
    "    sentences += review_to_sentences(review)\n",
    "\n",
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_NUM_FEATURES = 512   # Word vector dimensionality\n",
    "W2V_MIN_WORD_COUNT = 60   # Minimum word count\n",
    "W2V_NUM_WORKERS = 4      # Number of threads to run in parallel\n",
    "W2V_CONTEXT = 10          # Context window size\n",
    "W2V_DOWNSAMPLING = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=W2V_NUM_WORKERS, \\\n",
    "        vector_size=W2V_NUM_FEATURES, min_count = W2V_MIN_WORD_COUNT, \\\n",
    "        window = W2V_CONTEXT, sg = 1, sample = W2V_DOWNSAMPLING)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "model.save('model202203202347')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('./model202203202347')\n",
    "clear_output()\n",
    "# model.train(more_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "\n",
      "[('wonderful', 0.5646790862083435), ('fantastic', 0.54557865858078), ('terrific', 0.5346675515174866), ('excellent', 0.521806538105011), ('fine', 0.4908643662929535), ('superb', 0.4869849681854248), ('brilliant', 0.480425626039505), ('good', 0.46944496035575867), ('fabulous', 0.46233803033828735), ('outstanding', 0.45519980788230896)]\n",
      "\n",
      "model.wv.vectors.shape = (12625, 512)\n",
      "\n",
      "12625\n"
     ]
    }
   ],
   "source": [
    "print(f'{model.wv.doesnt_match(\"good great awesome bad\".split())}\\n') \n",
    "\n",
    "print(f'{model.wv.most_similar(\"great\")}\\n')\n",
    "print(f'model.wv.vectors.shape = {model.wv.vectors.shape}\\n')\n",
    "print(f'{len(model.wv.index_to_key)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordlist_to_vector(words, model):\n",
    "\n",
    "    wordVecList = []\n",
    "    wordSet = set(model.wv.index_to_key)\n",
    "\n",
    "    for word in words:\n",
    "        if word in wordSet:\n",
    "            wordVecList.append(model.wv[word])\n",
    "\n",
    "    if len(wordVecList) > 0:\n",
    "        return np.mean(wordVecList, axis=0)\n",
    "    else:\n",
    "        raise Exception('len(wordVecList) = 0')\n",
    "        return np.zeros((W2V_NUM_FEATURES,), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_to_vectors(reviews, model):\n",
    "    cleanWordLists = []\n",
    "    for review in reviews:\n",
    "        cleanWordLists.append(review_to_wordlist(review, remove_stopwords=True, lemmalization=True))\n",
    "    \n",
    "    vectors = []\n",
    "    vectorCount = 0\n",
    "    for cleanWordList in cleanWordLists:\n",
    "        vectors.append(wordlist_to_vector(cleanWordList, model))\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentiment', 'review'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "vec = wordlist_to_vector(sentences[0], model)\n",
    "\n",
    "train_df = labeled_df.drop(labels=['id'], axis=1)\n",
    "print(train_df.columns)\n",
    "labeled_x = train_df.drop(labels=['sentiment'], axis=1)\n",
    "labeled_y = train_df['sentiment']\n",
    "labeled_y.to_numpy()\n",
    "\n",
    "vectors = reviews_to_vectors(labeled_x['review'], model)\n",
    "vectors = np.stack(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest K-fold: 0.83736\n",
      "XGBoost K-fold: 0.8620000000000001\n"
     ]
    }
   ],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(vectors, labeled_y.to_numpy(), test_size=0.3, random_state=12345)\n",
    "\n",
    "all_x = vectors\n",
    "all_y = labeled_y.to_numpy()\n",
    "print()\n",
    "\n",
    "randomForest = RandomForestClassifier(n_estimators=100)\n",
    "xgboostModel = XGBClassifier(n_estimators=100, learning_rate=0.3, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "print(f'Random Forest K-fold: {cross_val_score(randomForest, all_x, all_y, cv=10, scoring=\"accuracy\").mean()}')\n",
    "print(f'XGBoost K-fold: {cross_val_score(xgboostModel, all_x, all_y, cv=10, scoring=\"accuracy\").mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest.fit(all_x, all_y)\n",
    "xgboostModel.fit(all_x, all_y)\n",
    "\n",
    "classifierModel = xgboostModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./testData.tsv', sep='\\t', header=0)\n",
    "test_x = reviews_to_vectors(test_df['review'], model)\n",
    "test_x = np.stack(test_x, axis=0)\n",
    "\n",
    "sample_df = pd.read_csv('./sampleSubmission.csv')\n",
    "sample_df['sentiment'] = np.squeeze(randomForest.predict(test_x))\n",
    "sample_df.to_csv('./sampleSubmission.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
